{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# back propogation, activation function and multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    #forward \n",
    "    #compute output value y given input x\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    #backward pass\n",
    "    #compute gradient for weight variables dE/dW\n",
    "    #and compute gradient for input dE/dx\n",
    "    #given 'node_grad' which is the gradient passed from the previous layers dE/dy\n",
    "    def backward(self, node_grad):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    #update weight variables based on gradient\n",
    "    def update(self, learn_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid Function\n",
    "\n",
    "$$y = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Gradients \n",
    "\n",
    "$$\\frac{\\partial{y}}{\\partial{x}}=y*(1-y)$$\n",
    "\n",
    "Backpropogation\n",
    "\n",
    "$$\\frac{\\partial{E}}{\\partial{x}}=\\frac{\\partial{E}}{\\partial{y}}*y*(1-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def forward(self, x):\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, node_grad):\n",
    "        return node_grad * self.y * (1 - self.y)\n",
    "    \n",
    "    def update(self, learn_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu Function\n",
    "\n",
    "$$\n",
    "y= \n",
    "\\begin{cases}\n",
    "    x, & \\text{if } x\\gt 0\\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Gradients \n",
    "\n",
    "$$\n",
    "\\frac{\\partial{y}}{\\partial{x}}= \n",
    "\\begin{cases}\n",
    "    1, & \\text{if } x\\gt 0\\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Backpropogation\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{x}}= \n",
    "\\begin{cases}\n",
    "    \\frac{\\partial{E}}{\\partial{y}}, & \\text{if } x\\gt 0\\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Layer):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, self.x)\n",
    "    \n",
    "    def backward(self, node_grad):\n",
    "        return node_grad * (self.x > 0)\n",
    "    \n",
    "    def update(self, learn_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax + CrossEntropy\n",
    "\n",
    "For forward and backward computation, please check https://deepnotes.io/softmax-crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_Cross_Entropy(Layer):\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x - np.max(x))\n",
    "        self.y = exps / np.sum(exps)\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, label):\n",
    "        self.out_grad = self.y - label\n",
    "        return self.out_grad\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Layer\n",
    "\n",
    "For forward and backward computation, please check http://cs231n.stanford.edu/handouts/linear-backprop.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, size_in, size_out, with_bias):\n",
    "        self.size_in = size_in\n",
    "        self.size_out = size_out\n",
    "        self.with_bias = with_bias\n",
    "        self.W = self.initialize_weight()\n",
    "        if with_bias:\n",
    "            self.b = np.zeros(size_out)\n",
    "        \n",
    "    def initialize_weight(self):\n",
    "        epsilon = np.sqrt(2.0 / (self.size_in + self.size_out))\n",
    "        return epsilon * (np.random.rand(self.size_in, self.size_out) * 2 - 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = np.dot(x, self.W) + self.b\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, node_grad):\n",
    "        self.G_W = np.outer(self.x, node_grad)\n",
    "        if self.with_bias:\n",
    "            self.G_b = node_grad\n",
    "        return node_grad @ self.W.T\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        self.W -= learning_rate * self.G_W\n",
    "        if self.with_bias:\n",
    "            self.b -= learning_rate * self.G_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Layer):\n",
    "    def __init__(self, layer_size, with_bias=True, activation=\"sigmoid\", learning_rate=1):\n",
    "        assert len(layer_size) >= 2\n",
    "        self.layer_size = layer_size\n",
    "        self.with_bias = with_bias\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = Sigmoid\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = Relu\n",
    "        else:\n",
    "            raise Exception(\"activation not implemented\")\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.layers = []\n",
    "        \n",
    "        size_in = self.layer_size[0]\n",
    "        for hu in self.layer_size[1:-1]:\n",
    "            self.layers.append(Linear(size_in, hu, self.with_bias))\n",
    "            self.layers.append(self.activation())\n",
    "            size_in = hu\n",
    "            \n",
    "        # final layer uses softmax+crossentropy\n",
    "        self.layers.append(Linear(size_in, self.layer_size[-1], self.with_bias))\n",
    "        self.layers.append(Softmax_Cross_Entropy())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, label):\n",
    "        node_grad = label\n",
    "        for layer in reversed(self.layers):\n",
    "            node_grad = layer.backward(node_grad)\n",
    "            \n",
    "    def update(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "            \n",
    "    def train(self, x, label):\n",
    "        y = self.forward(x)\n",
    "        self.backward(label)\n",
    "        self.update(self.learning_rate)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return np.argmax(x)\n",
    "    \n",
    "    def loss(self, x, label):\n",
    "        y = self.forward(x)\n",
    "        return -np.log(y) @ label # cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "Y = np.array([\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 0],\n",
    "    [0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1127318d0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1007) # set random seed\n",
    "EPOCH = 1000 # set training epochs\n",
    "N = X.shape[0] # get number of training data\n",
    "\n",
    "# craete a MLP with layer size [2,4,2] and learning_rate 0.1\n",
    "mlp = MLP([2, 4, 2], learning_rate=.1, activation=\"relu\")\n",
    "\n",
    "loss = np.zeros(EPOCH) # store losses\n",
    "for epoch in range(EPOCH):\n",
    "    # train on each training data\n",
    "    for i in range(N):\n",
    "        mlp.train(X[i], Y[i])\n",
    "        \n",
    "    # compute loss\n",
    "    for i in range(N):\n",
    "        loss[epoch] += mlp.loss(X[i], Y[i])\n",
    "        \n",
    "    loss[epoch] /= N\n",
    "    \n",
    "# plot loss curve\n",
    "plt.figure()\n",
    "ix = np.arange(EPOCH)\n",
    "plt.plot(ix, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
