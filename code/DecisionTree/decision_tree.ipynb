{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "---\n",
    "## Overview\n",
    "* **Task:** Using dataset of titanic: machine learning from disaster (you can check https://www.kaggle.com/c/titanic/data for details) to train a decision tree model so that it predicts whether a passenger can survive given corresponding imformation in type of category, text, and number.\n",
    "* **Input:** train_file with labels: ../input/titanic/train.csv & test_file without labels: ../input/titanic/test.csv\n",
    "* **Label:**\n",
    "\n",
    "| Index | Variable   | Definition                                                   | Type |\n",
    "| ----- | ---------- | ------------------------------------------------------------ | ---- |\n",
    "| 0     | 'Pclass'   | Passenger's class (1st, 2nd, or 3rd)                         | cat  |\n",
    "| 1     | 'Name'     | Passenger's name                                             | text |\n",
    "| 2     | 'Sex'      | Passenger's sex                                              | cat  |\n",
    "| 3     | 'Age'      | Passenger's age                                              | num  |\n",
    "| 4     | 'SibSp'    | Number of siblings/spouses aboard the Titanic                | cat  |\n",
    "| 5     | 'Parch'    | Number of parents/children aboard the Titanic                | cat  |\n",
    "| 6     | 'Ticket'   | Ticket number                                                | text |\n",
    "| 7     | 'Fare'     | Fare paid for ticket                                         | num  |\n",
    "| 8     | 'Cabin'    | Cabin number                                                 | set  |\n",
    "| 9     | 'Embarked' | Where the passenger got on the ship (C - Cherbourg, S - Southampton, Q = Queenstown) | cat  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_FEAT = [0, 2, 4, 5, 9]\n",
    "NUM_FEAT = [3, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field_names: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "field_names: ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "train_feat:\n",
      " ['3' '\"Braund, Mr. Owen Harris\"' 'male' '22' '1' '0' 'A/5 21171' '7.25' ''\n",
      " 'S']\n",
      "test_feat:\n",
      " ['3' '\"Kelly, Mr. James\"' 'male' '34.5' '0' '0' '330911' '7.8292' '' 'Q']\n",
      "train_feat:\n",
      " ['3' 'None' 'male' '22' '1' '0' 'None' '7.25' 'None' 'S']\n",
      "test_feat:\n",
      " ['3' 'None' 'male' '34.5' '0' '0' 'None' '7.8292' 'None' 'Q']\n"
     ]
    }
   ],
   "source": [
    "# parse a string into fields, skip quotations\n",
    "def parse_feat(line):\n",
    "    quota = False\n",
    "    j = 0\n",
    "    feats = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i] == '\\\"':\n",
    "            quota = not quota\n",
    "        if line[i] == ',' and not quota:\n",
    "            feat = line[j:i]\n",
    "            feats.append(feat)\n",
    "            j = i+1\n",
    "    return feats + [line[j:]]\n",
    "\n",
    "\n",
    "# load a csv file, use parse_feat() to convert format\n",
    "def load_file(file_name):\n",
    "    data = []\n",
    "    with open(file_name, 'r') as fin:\n",
    "        print('field_names:', fin.readline().strip().split(','))\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            data.append(parse_feat(line))\n",
    "    return np.array(data)\n",
    "\n",
    "train_data = load_file('../../data/train.csv')\n",
    "test_data = load_file('../../data/test.csv')\n",
    "\n",
    "train_id, train_label, train_feat = train_data[:, 0], train_data[:, 1], train_data[:, 2:]\n",
    "test_id, test_feat = test_data[:, 0], test_data[:, 1:]\n",
    "\n",
    "print('train_feat:\\n', train_feat[0])\n",
    "print('test_feat:\\n', test_feat[0])\n",
    "\n",
    "train_feat[:, [1, 6, 8]] = None\n",
    "test_feat[:, [1, 6, 8]] = None\n",
    "\n",
    "print('train_feat:\\n', train_feat[0])\n",
    "print('test_feat:\\n', test_feat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART classification tree can deal with continuous numerical features by discretization.\n",
    "\n",
    "Suppose there are $m$ samples, where the continuous feature $A$ has $m$ different values, sorted by $a_1, a_2, \\dots, a_m$.\n",
    "\n",
    "1. Taking the average of two adjacent samples as the dividing point, and obtaining $m-1$ dividing points\n",
    "\n",
    "2. The data is divided into two categories at each division point, and the Gini coefficient is calculated.\n",
    "\n",
    "3. Select the point with the smallest Gini coefficient as the dividing point of the continuous feature\n",
    "\n",
    "In this example, we would like to implement basic ID3 classification tree, so we discretize the continuous features in a similar way during data preprocessing as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17   8.153 16.136 24.119 32.102 40.085 48.068 56.051 64.034 72.017]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.       51.23292 102.46584 153.69876 204.93168 256.1646  307.39752\n",
      " 358.63044 409.86336 461.09628]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# return a separator to categorize a numerical field\n",
    "def num2cat(num_feat, n_class=10):\n",
    "    def to_float(x):\n",
    "        if len(x):\n",
    "            return float(x)\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    num_feat = np.array([to_float(x) for x in num_feat])\n",
    "    min_val, max_val = num_feat[num_feat > -1].min(), num_feat.max()\n",
    "    sep = np.linspace(min_val, max_val, n_class, endpoint=False)\n",
    "    print(sep)\n",
    "    plt.hist(num_feat[num_feat > -1], bins=n_class) \n",
    "    plt.show()\n",
    "\n",
    "    def indicator(x):\n",
    "        x = to_float(x)\n",
    "        if x == -1:\n",
    "            return 0\n",
    "        for i in range(len(sep)):\n",
    "            if x < sep[i]:\n",
    "                return i    \n",
    "        return n_class\n",
    "\n",
    "    return indicator\n",
    "\n",
    "\n",
    "for nf in NUM_FEAT:\n",
    "    ind = num2cat(list(train_feat[:, nf]) + list(test_feat[:, nf]))\n",
    "    for _ in range(len(train_feat[:, nf])):\n",
    "        train_feat[_, nf] = str(ind(train_feat[_, nf]))\n",
    "    for _ in range(len(test_feat[:, nf])):\n",
    "        test_feat[_, nf] = str(ind(test_feat[_, nf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_feat:\n",
      " [['3' 'male' '3' ... '0' '1' 'S']\n",
      " ['1' 'female' '5' ... '0' '2' 'C']\n",
      " ['3' 'female' '4' ... '0' '1' 'S']\n",
      " ...\n",
      " ['3' 'female' '0' ... '2' '1' 'S']\n",
      " ['1' 'male' '4' ... '0' '1' 'C']\n",
      " ['3' 'male' '4' ... '0' '1' 'Q']]\n",
      "test_feat:\n",
      " [['3' 'male' '5' ... '0' '1' 'Q']\n",
      " ['3' 'female' '6' ... '0' '1' 'S']\n",
      " ['2' 'male' '8' ... '0' '1' 'Q']\n",
      " ...\n",
      " ['3' 'male' '5' ... '0' '1' 'S']\n",
      " ['3' 'male' '0' ... '0' '1' 'S']\n",
      " ['3' 'male' '0' ... '1' '1' 'C']]\n"
     ]
    }
   ],
   "source": [
    "train_feat = np.delete(train_feat, [1, 6, 8], axis=1)\n",
    "test_feat = np.delete(test_feat, [1, 6, 8], axis=1)\n",
    "\n",
    "print('train_feat:\\n', train_feat)\n",
    "print('test_feat:\\n', test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_feat:\n",
      " [[ 0  3  5 ... 23 31 39]\n",
      " [ 1  4  6 ... 23 32 40]\n",
      " [ 0  4  7 ... 23 31 39]\n",
      " ...\n",
      " [ 0  4  8 ... 25 31 39]\n",
      " [ 1  3  7 ... 23 31 40]\n",
      " [ 0  3  7 ... 23 31 41]]\n",
      "test_feat:\n",
      " [[ 0  3  6 ... 23 31 41]\n",
      " [ 0  4 14 ... 23 31 39]\n",
      " [ 2  3 12 ... 23 31 41]\n",
      " ...\n",
      " [ 0  3  6 ... 23 31 39]\n",
      " [ 0  3  8 ... 23 31 39]\n",
      " [ 0  3  8 ... 24 31 40]]\n"
     ]
    }
   ],
   "source": [
    "class Dataset:\n",
    "    # build feature map for one-hot encoding\n",
    "    @staticmethod\n",
    "    def build_feat_map(cat_feats):\n",
    "        feat_map = {}\n",
    "        for i in range(cat_feats.shape[1]):\n",
    "            for x in cat_feats[:, i]:\n",
    "                feat_name = str(i) + ':' + x\n",
    "                if feat_name not in feat_map:\n",
    "                    feat_map[feat_name] = len(feat_map)\n",
    "\n",
    "        return feat_map\n",
    "    \n",
    "    # one-hot encoding\n",
    "    def feat2id(self, cat_feats):\n",
    "        feat_ids = []\n",
    "        for i in range(cat_feats.shape[1]):\n",
    "            feat_ids.append([])\n",
    "            for x in cat_feats[:, i]:\n",
    "                feat_name = str(i) + ':' + x\n",
    "                feat_ids[-1].append(self.feat_map[feat_name])\n",
    "        return np.int32(feat_ids).transpose()\n",
    "    \n",
    "    # split validation set\n",
    "    def split_train_valid(self):\n",
    "        np.random.seed(123)\n",
    "        rnd = np.random.random(len(self.train_label))\n",
    "        self.train_ind = np.where(rnd < 0.8)[0]\n",
    "        self.valid_ind = np.where(rnd >= 0.8)[0]\n",
    "\n",
    "        def to_csr(data, dim=len(self.feat_map)):\n",
    "            row = np.zeros_like(data) + np.expand_dims(np.arange(len(data)), 1)\n",
    "            val = np.ones_like(data)\n",
    "            return csr_matrix((val.flatten(), (row.flatten(), data.flatten())), shape=(len(data), dim))    \n",
    "            \n",
    "        self.train_data = (self.train_label[self.train_ind], to_csr(self.train_feat[self.train_ind]))\n",
    "        self.valid_data = (self.train_label[self.valid_ind], to_csr(self.train_feat[self.valid_ind]))\n",
    "        self.test_data = (np.zeros(len(self.test_feat), dtype=np.int32), to_csr(self.test_feat))\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feat_map = self.build_feat_map(np.vstack([train_feat, test_feat]))\n",
    "\n",
    "        self.train_id, self.test_id = train_id, test_id\n",
    "        self.train_label = np.int32(train_label)\n",
    "        self.train_feat, self.test_feat = self.feat2id(train_feat), self.feat2id(test_feat)\n",
    "\n",
    "        print('train_feat:\\n', self.train_feat)\n",
    "        print('test_feat:\\n', self.test_feat)\n",
    "\n",
    "        self.split_train_valid()\n",
    "        \n",
    "Data = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_feat:\n",
      " [[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 1 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 1 0 0]\n",
      " [1 0 0 ... 0 1 0]]\n",
      "valid_feat:\n",
      " [[0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "train_label, train_feat = Data.train_data[0], Data.train_data[1].toarray()\n",
    "valid_label, valid_feat = Data.valid_data[0], Data.valid_data[1].toarray()\n",
    "\n",
    "print('train_feat:\\n', train_feat)\n",
    "print('valid_feat:\\n', valid_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implemention of Decision Tree Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(X)=-\\sum_{i=1}^{n}p_i\\log{p_i}$$\n",
    "\n",
    "$$H(X|Y)=\\sum_{i=1}^{n}p(Y=y_i)H(X|Y=y_i)$$\n",
    "\n",
    "$$g(X,Y)=H(X)-H(X|Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8231966053748232\n",
      "0.7717391304347826\n"
     ]
    }
   ],
   "source": [
    "NID = {}\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feat_id=-1):\n",
    "        self.feat_id = feat_id\n",
    "        self.nid = len(NID)\n",
    "        NID[self.nid] = self\n",
    "        self.t_child = None\n",
    "        self.f_child = None\n",
    "        self._class = -1\n",
    "        \n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, n_feat, max_depth=6, verbose=True):\n",
    "        self.n_feat = n_feat\n",
    "        self.max_depth = max_depth\n",
    "        self.verbose = verbose\n",
    "        self.root_node = Node()\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(labels):\n",
    "        p = np.sum(labels) / len(labels)\n",
    "        if p == 0 or p == 1:\n",
    "            return 0\n",
    "        return - p * np.log(p) - (1-p) * np.log(1-p)\n",
    "\n",
    "    def fit(self, labels, data, cur_node=None, cur_depth=1):\n",
    "        if cur_node is None:\n",
    "            cur_node = self.root_node\n",
    "\n",
    "        if self.verbose:\n",
    "            print(cur_node.nid)\n",
    "        \n",
    "        if labels.sum() == len(labels):\n",
    "            cur_node._class = 1\n",
    "            cur_node.t_child = None\n",
    "            cur_node.f_child = None\n",
    "            return\n",
    "        elif labels.sum() == 0:\n",
    "            cur_node._class = 0\n",
    "            cur_node.t_child = None\n",
    "            cur_node.f_child = None\n",
    "            return\n",
    "        elif cur_depth == self.max_depth:\n",
    "            cur_node._class = labels.sum() / len(labels) >= 0.5\n",
    "            cur_node.t_child = None\n",
    "            cur_node.f_child = None\n",
    "            return\n",
    "        \n",
    "        base_ent = self.entropy(labels)\n",
    "        info_gain = 0\n",
    "        best_split = None\n",
    "        best_t_ind = None\n",
    "        best_f_ind = None\n",
    "        \n",
    "        csc_data = data.tocsc()\n",
    "        for f in range(self.n_feat):\n",
    "            feat = csc_data[:, f].toarray().flatten()\n",
    "            t_ind = feat == 1\n",
    "            f_ind = feat == 0\n",
    "            f_ent = base_ent\n",
    "            if t_ind.sum():\n",
    "                f_ent -= t_ind.sum() / len(feat) * self.entropy(labels[t_ind])\n",
    "            if f_ind.sum():\n",
    "                f_ent -= f_ind.sum() / len(feat) * self.entropy(labels[f_ind])\n",
    "            if info_gain < f_ent:\n",
    "                info_gain = f_ent\n",
    "                best_split = f\n",
    "                best_t_ind = t_ind\n",
    "                best_f_ind = f_ind\n",
    "                \n",
    "        if info_gain == 0:\n",
    "            cur_node._class = labels.sum() / len(labels) >= 0.5\n",
    "            cur_node.t_child = None\n",
    "            cur_node.f_child = None\n",
    "            return\n",
    "                \n",
    "        cur_node.feat_id = best_split\n",
    "        cur_node.t_child = Node()\n",
    "        cur_node.f_child = Node()\n",
    "        \n",
    "        self.fit(labels[best_t_ind], data[best_t_ind], cur_node.t_child, cur_depth+1)\n",
    "        self.fit(labels[best_f_ind], data[best_f_ind], cur_node.f_child, cur_depth+1)\n",
    "\n",
    "    def predict(self, data):\n",
    "        assert data.ndim == 1\n",
    "        cur_node = self.root_node\n",
    "        feat_set = set(data)\n",
    "\n",
    "        while True:\n",
    "            if cur_node.t_child is None or cur_node.f_child is None:\n",
    "                return cur_node._class\n",
    "            if cur_node.feat_id in feat_set:\n",
    "                cur_node = cur_node.t_child\n",
    "            else:\n",
    "                cur_node = cur_node.f_child\n",
    "\n",
    "    def batch_predict(self, data):\n",
    "        preds = []\n",
    "        for i in range(data.shape[0]):\n",
    "            preds.append(self.predict(data[i].tocoo().col))\n",
    "        return np.array(preds)\n",
    "\n",
    "    def acc(self, labels, data):\n",
    "        preds = self.batch_predict(data)\n",
    "        acc = np.int32(labels == preds).sum() / len(labels)\n",
    "        return acc\n",
    "\n",
    "DT = DecisionTree(len(Data.feat_map), max_depth=5, verbose=False)   #超参调节max_depth\n",
    "DT.fit(*Data.train_data)\n",
    "\n",
    "print(DT.acc(*Data.train_data))\n",
    "print(DT.acc(*Data.valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implemention with Sklearn Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_feat:\n",
      " [[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 1 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 1 0 0]\n",
      " [1 0 0 ... 0 1 0]]\n",
      "valid_feat:\n",
      " [[0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "train_label, train_feat = Data.train_data[0], Data.train_data[1].toarray()\n",
    "valid_label, valid_feat = Data.valid_data[0], Data.valid_data[1].toarray()\n",
    "\n",
    "print('train_feat:\\n', train_feat)\n",
    "print('valid_feat:\\n', valid_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.8769448373408769\n",
      "valid acc: 0.8043478260869565\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=10)\n",
    "clf.fit(train_feat, train_label)\n",
    "\n",
    "train_preds = clf.predict(train_feat)\n",
    "valid_preds = clf.predict(valid_feat)\n",
    "print('train acc:', np.sum(train_preds == train_label) / len(train_label))\n",
    "print('valid acc:', np.sum(valid_preds == valid_label) / len(valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualization of Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydotplus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-59b3ec01f4c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# graphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pydotplus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydotplus'"
     ]
    }
   ],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "import pydotplus\n",
    "\n",
    "# graphviz\n",
    "!pip install pydotplus\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf,\n",
    "                    out_file=dot_data,\n",
    "                    feature_names=sorted(sorted(Data.feat_map.keys())),\n",
    "                    class_names=['non-survival', 'survival'],\n",
    "                    filled=True, rounded=True,\n",
    "                    impurity=False)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "graph.write_pdf('viz.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
